---
title: "HW3"
author: "Jianan Zhang"
format: pdf
jupyter: stat4255
---

# Exercise 1

## (a)
- **Training performance:**  
  QDA is more flexible and typically captures training patterns more closely, even when the actual decision boundary is linear. As a result, it generally achieves **lower training error**.

- **Test performance:**  
  When the true decision boundary is linear, LDA’s assumptions are correct, while QDA introduces extra variance by estimating separate covariance matrices. Therefore, **LDA usually yields a smaller test error** in this case.

## (b)
- **Training:**  
  QDA’s quadratic boundaries allow it to adapt to nonlinear relationships, giving it **lower training error** than LDA.

- **Testing:**  
  If the sample size is large enough to estimate QDA’s parameters reliably, it tends to **outperform LDA** on the test set as well. However, with limited data, the extra flexibility of QDA may lead to overfitting.

## (c)
As the sample size \(n\) increases, parameter estimates in QDA (especially the covariance matrices) become more accurate. Consequently, the **variance decreases**, and the relative **test accuracy of QDA improves** compared to LDA.  
In small samples, QDA often overfits due to high variance, but with large \(n\), its flexibility becomes advantageous.

## (d)
**False.**  
Although QDA can model a linear decision boundary as a special case, its higher variance means that when the true Bayes boundary is linear, **LDA** is typically better.  
LDA’s simpler form has lower variance and correct bias, so the additional flexibility of QDA offers no benefit here.

---

# Exercise 2

```{python}
# --- Libraries ---
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score

import ISLP as islp
Auto = islp.load_data("Auto")
```

## (a)
```{python}
mpg_median = Auto['mpg'].median()
Auto['mpg01'] = (Auto['mpg'] > mpg_median).astype(int)

```

## (b)
```{python}
cols = ['cylinders','displacement','horsepower','weight','acceleration','year','origin']

for feature in cols:
    plt.figure(figsize=(5,4))
    sns.boxplot(x='mpg01', y=feature, data=Auto)
    plt.title(f'{feature} vs mpg01')
    plt.tight_layout()
    plt.show()

sns.pairplot(
    Auto,
    vars=['displacement','horsepower','weight','acceleration','year'],
    hue='mpg01',
    diag_kind='hist'
)
plt.show()
```

Interpretation:
Cars with higher mpg tend to have smaller displacement, lower horsepower, and lighter weight.
Model year is positively associated with mpg, while acceleration differs little.
Thus, displacement, horsepower, weight, and year appear most useful for classification.

## (c)
```{python}
X = Auto[['cylinders','displacement','horsepower','weight','acceleration','year','origin']]
y = Auto['mpg01']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1
)

```

## (d)
```{python}
predictors = ['cylinders','displacement','horsepower','weight','year']
lda = LinearDiscriminantAnalysis()
lda.fit(X_train[predictors], y_train)

y_pred = lda.predict(X_test[predictors])
acc = accuracy_score(y_test, y_pred)

print("Test Accuracy:", round(acc,3))
print("Test Error:", round(1-acc,3))

```

## (e)
```{python}
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train[predictors], y_train)

y_pred_qda = qda.predict(X_test[predictors])
acc_qda = accuracy_score(y_test, y_pred_qda)

print("Test Accuracy:", round(acc_qda,3))
print("Test Error:", round(1-acc_qda,3))

```

## (f)
```{python}
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train[predictors], y_train)

y_pred_log = log_model.predict(X_test[predictors])
acc_log = accuracy_score(y_test, y_pred_log)

print("Test Accuracy:", round(acc_log,3))
print("Test Error:", round(1-acc_log,3))

```

## (g)
```{python}
nb_model = GaussianNB()
nb_model.fit(X_train[predictors], y_train)

y_pred_nb = nb_model.predict(X_test[predictors])
acc_nb = accuracy_score(y_test, y_pred_nb)

print("Test Accuracy:", round(acc_nb,3))
print("Test Error:", round(1-acc_nb,3))

```

## (h)
```{python}
test_errs = []
for k in [1,3,5,7,9,11,15,20]:
    knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))
    knn.fit(X_train[predictors], y_train)
    pred = knn.predict(X_test[predictors])
    err = 1 - accuracy_score(y_test, pred)
    test_errs.append((k, round(err,3)))

for k, e in test_errs:
    print(f"K = {k:2d}, Test Error = {e}")

```

# Exercise3
```{python}
from palmerpenguins import load_penguins
penguins = load_penguins()
for col in ['species','island','sex']:
    penguins[col] = penguins[col].astype('category')
penguins

```

## (1)
```{python}
len(penguins)
penguins['species'].nunique()
penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']].isna().sum()

```

## (2)
```{python}
penguins_clean = penguins.dropna(
    subset=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']
)
print("Before:", len(penguins))
print("After:", len(penguins_clean))

```

## (3)
```{python}
sns.pairplot(
    penguins_clean,
    vars=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'],
    hue='species',
    palette='Set2',
    diag_kind='hist'
)
plt.suptitle("Scatterplot Matrix of Penguin Measurements", y=1.02)
plt.show()
```
Observation:
Bill length vs. depth and flipper length vs. body mass effectively separate the three species.
Adelie = shorter/deeper bill; Gentoo = longer/shallow bill and heavier body; Chinstrap = intermediate.

## (4)
```{python}
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score

features = ['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']
X = penguins_clean[features]
y = penguins_clean['species']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1, stratify=y
)

lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
y_pred = lda.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print("Test Accuracy:", round(acc,3))
print("Test Error:", round(1-acc,3))

```

## (5)
```{python}
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1, stratify=y
)

lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

classes = lda.classes_
y_train_bin = label_binarize(y_train, classes=classes)
proba_train = lda.predict_proba(X_train)

plt.figure(figsize=(7,6))
for i, sp in enumerate(classes):
    fpr, tpr, _ = roc_curve(y_train_bin[:, i], proba_train[:, i])
    roc_auc = auc(fpr, tpr)
    print(f"TRAIN {sp} AUC = {roc_auc:.3f}")
    plt.plot(fpr, tpr, lw=2, label=f'{sp} (AUC={roc_auc:.2f})')

plt.plot([0,1],[0,1],'k--',lw=1)
plt.title("ROC Curves for LDA (Training Set, One-vs-Rest)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()
```
Interpretation:
All ROC curves lie near the top-left, showing almost perfect class separation with AUC ≈ 1.


## Exercise4
```{python}
import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

GLOW = pd.read_csv("http://knightgu.github.io/data/GLOW.data", sep="\\s+")
cols = ['PRIORFRAC','PREMENO','MOMFRAC','ARMASSIST','SMOKE','RATERISK','FRACTURE']
GLOW[cols] = GLOW[cols].astype('category')
GLOW
```

## (1)
```{python}
X = GLOW[['AGE','WEIGHT','PRIORFRAC','PREMENO','RATERISK']]
y = GLOW['FRACTURE'].cat.codes

num_cols = ['AGE','WEIGHT']
cat_cols = ['PRIORFRAC','PREMENO','RATERISK']

preprocessor = ColumnTransformer([
    ('categorical', OneHotEncoder(drop='first'), cat_cols),
    ('numerical', 'passthrough', num_cols)
])

log_reg = Pipeline([
    ('prep', preprocessor),
    ('clf', LogisticRegression(max_iter=1000))
])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

log_reg.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

feat_names = log_reg.named_steps['prep'].named_transformers_['categorical'].get_feature_names_out(cat_cols)
feat_names = np.concatenate([feat_names, num_cols])

coefs = log_reg.named_steps['clf'].coef_[0]
coef_df = pd.DataFrame({
    'Feature': feat_names,
    'Coefficient': coefs,
    'OddsRatio': np.exp(coefs)
}).sort_values('OddsRatio', ascending=False)
coef_df

```

## (2)
Women with a previous fracture (PRIORFRAC = Yes) have substantially higher odds of experiencing another fracture.
For RATERISK, both “Same” and “Greater” categories show higher odds than “Less,” suggesting perceived risk aligns with actual outcomes.

## (3)
```{python}
GLOW = GLOW.dropna(subset=['AGE','PRIORFRAC','RATERISK','FRACTURE'])
GLOW['FRACTURE_NUM'] = GLOW['FRACTURE'].map({'No':0,'Yes':1}).astype(int)
GLOW['PRIORFRAC'] = GLOW['PRIORFRAC'].astype('category')
GLOW['RATERISK'] = GLOW['RATERISK'].astype('category')

reduced = smf.logit(
    formula="FRACTURE_NUM ~ AGE + PRIORFRAC + C(RATERISK, Treatment('Less'))",
    data=GLOW
).fit()

print(reduced.summary())

sample = pd.DataFrame({'AGE':[65],'PRIORFRAC':['Yes'],'RATERISK':['Same']})
pred = float(reduced.predict(sample))
print(f"Predicted probability: {pred:.3f}")

```